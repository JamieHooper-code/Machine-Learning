{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DataFrame Creation\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df_from_csv = pd.read_csv('file_path.csv')\n",
    "df_from_excel = pd.read_excel('file_path.xlsx')\n",
    "\n",
    "# Basic Information\n",
    "shape = df.shape\n",
    "info = df.info()\n",
    "summary = df.describe()\n",
    "data_types = df.dtypes\n",
    "df.columns\n",
    "\n",
    "# Selection and Indexing\n",
    "col_a = df['A']\n",
    "subset = df[['A', 'B']]\n",
    "row_by_index = df.loc[0]\n",
    "row_by_position = df.iloc[0]\n",
    "\n",
    "# Filtering\n",
    "filtered = df[df['A'] > 1]\n",
    "\n",
    "# Adding/Removing Columns\n",
    "df['C'] = df['A'] + df['B']\n",
    "df.drop('A', axis=1, inplace=True)\n",
    "\n",
    "# Handling Missing Data\n",
    "df.dropna(inplace=True)\n",
    "df.fillna(value=0, inplace=True)\n",
    "\n",
    "# Grouping and Aggregation\n",
    "grouped = df.groupby('A')\n",
    "agg = df.groupby('A').agg({'B': 'sum'})\n",
    "\n",
    "# Merging, Joining, and Concatenating\n",
    "df2 = pd.DataFrame({'A': [4, 5, 6], 'D': [7, 8, 9]})\n",
    "merged = pd.merge(df, df2, on='A', how='inner')\n",
    "joined = df.join(df2, lsuffix='_left', rsuffix='_right')\n",
    "concatenated = pd.concat([df, df2], axis=0)\n",
    "\n",
    "# DateTime Operations\n",
    "df['date'] = pd.to_datetime(df['date_string'])\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Setting and Resetting Index\n",
    "df.set_index('A', inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Applying Functions\n",
    "df['E'] = df['A'].apply(lambda x: x*2)\n",
    "df['F'] = df.apply(lambda row: row['A'] + row['B'], axis=1)\n",
    "\n",
    "# Sorting\n",
    "df.sort_values(by='B', ascending=False, inplace=True)\n",
    "\n",
    "# Saving to File\n",
    "df.to_csv('output.csv', index=False)\n",
    "df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "# Miscellaneous\n",
    "unique_values = df['A'].unique()\n",
    "value_counts = df['A'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS ANALYZING DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5, 5],\n",
    "    'B': [5, 4, 3, 2, 1, 1],\n",
    "    'C': ['p', 'q', 'r', 's', 't', 't']\n",
    "})\n",
    "\n",
    "# 1. Basic Descriptive Statistics\n",
    "\n",
    "# Get summary statistics for numerical columns\n",
    "summary = df.describe()\n",
    "\n",
    "# Mean of a column\n",
    "mean_a = df['A'].mean()\n",
    "\n",
    "# Median of a column\n",
    "median_a = df['A'].median()\n",
    "\n",
    "# Mode of a column\n",
    "mode_a = df['A'].mode()\n",
    "\n",
    "# 2. Value Counts and Unique Values\n",
    "\n",
    "# Count of unique values in a column\n",
    "value_counts_c = df['C'].value_counts()\n",
    "\n",
    "# List of unique values\n",
    "unique_c = df['C'].unique()\n",
    "\n",
    "# Number of unique values\n",
    "nunique_c = df['C'].nunique()\n",
    "\n",
    "# 3. Correlation and Covariance\n",
    "\n",
    "# Correlation matrix\n",
    "correlation = df.corr()\n",
    "\n",
    "# Covariance matrix\n",
    "covariance = df.cov()\n",
    "\n",
    "# 4. Grouping and Aggregation\n",
    "\n",
    "# Group by a column and calculate mean of other columns\n",
    "grouped = df.groupby('C').mean()\n",
    "\n",
    "# Multiple aggregation functions\n",
    "agg_funcs = df.groupby('C').agg({'A': ['mean', 'sum'], 'B': 'max'})\n",
    "\n",
    "# 5. Handling Missing Data\n",
    "\n",
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Fill missing values with a default value (e.g., 0)\n",
    "df_filled = df.fillna(0)\n",
    "\n",
    "# 6. Cross Tabulation\n",
    "\n",
    "# Cross tabulation of two columns\n",
    "cross_tab = pd.crosstab(df['A'], df['C'])\n",
    "\n",
    "# 7. Pivot Table\n",
    "\n",
    "# Create a pivot table\n",
    "pivot = df.pivot_table(values='A', index='C', aggfunc='mean')\n",
    "\n",
    "# 8. Histograms\n",
    "\n",
    "# Histogram of a column\n",
    "hist_a = df['A'].hist(bins=10)\n",
    "\n",
    "# 9. Cumulative Sum\n",
    "\n",
    "# Cumulative sum of a column\n",
    "cumsum_a = df['A'].cumsum()\n",
    "\n",
    "# 10. Rolling Statistics\n",
    "\n",
    "# Rolling mean over 3 periods\n",
    "rolling_mean = df['A'].rolling(window=3).mean()\n",
    "\n",
    "# 11. Rank\n",
    "\n",
    "# Rank of values in a column\n",
    "rank_a = df['A'].rank()\n",
    "\n",
    "# 12. Percentage Change\n",
    "\n",
    "# Percentage change between the current and a prior element\n",
    "pct_change = df['A'].pct_change()\n",
    "\n",
    "# 13. Value Counts (Normalized)\n",
    "\n",
    "# Normalized value counts (percentage)\n",
    "value_counts_norm = df['C'].value_counts(normalize=True)\n",
    "\n",
    "# 14. Detecting Outliers (using Z-score)\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = (df['A'] - df['A'].mean()) / df['A'].std()\n",
    "outliers = df[z_scores.abs() > 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/survey_results_public.csv', index_col='Respondent')\n",
    "schema_df = pd.read_csv('data/survey_results_schema.csv', index_col='Column')\n",
    "\n",
    "df.set_index('Respondent')\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "    'D': ['D0', 'D1', 'D2', 'D3']\n",
    "}, index=['X', 'Y', 'Z', 'W'])\n",
    "\n",
    "# 1. Basic Indexing\n",
    "\n",
    "# Select column\n",
    "col_a = df['A']\n",
    "\n",
    "# Select multiple columns\n",
    "subset = df[['A', 'B']]\n",
    "\n",
    "# 2. Using .loc (label based indexing)\n",
    "\n",
    "# Select a row by index label\n",
    "row_x = df.loc['X']\n",
    "\n",
    "# Select multiple rows by index labels\n",
    "rows_xy = df.loc[['X', 'Y']]\n",
    "\n",
    "# Select a specific value\n",
    "val_xy = df.loc['X', 'Y']\n",
    "\n",
    "# Select a slice of rows for specific columns\n",
    "subset_loc = df.loc['X':'Z', ['A', 'B']]\n",
    "\n",
    "# 3. Using .iloc (integer-location based indexing)\n",
    "\n",
    "# Select the first row\n",
    "row_0 = df.iloc[0]\n",
    "\n",
    "# Select the first and second row\n",
    "rows_01 = df.iloc[0:2]\n",
    "\n",
    "# Select a specific value\n",
    "val_01 = df.iloc[0, 1]\n",
    "\n",
    "# Select a slice of rows for specific columns\n",
    "subset_iloc = df.iloc[0:2, 0:2]\n",
    "\n",
    "# 4. Boolean Indexing\n",
    "\n",
    "# Filter rows based on column values\n",
    "filtered = df[df['A'] == 'A0']\n",
    "\n",
    "# Using `isin` for filtering\n",
    "filtered_isin = df[df['A'].isin(['A0', 'A1'])]\n",
    "\n",
    "# 5. MultiIndex (Hierarchical Indexing)\n",
    "\n",
    "arrays = [['A', 'A', 'B', 'B'], [1, 2, 1, 2]]\n",
    "multi_index = pd.MultiIndex.from_arrays(arrays, names=('letters', 'numbers'))\n",
    "df_multi = pd.DataFrame({'data': [10, 20, 30, 40]}, index=multi_index)\n",
    "\n",
    "# Selecting data with multi-index\n",
    "subset_multi = df_multi.loc['A']\n",
    "subset_multi_specific = df_multi.loc[('A', 1)]\n",
    "\n",
    "# 6. Setting and Resetting Index\n",
    "\n",
    "# Set a column as the index\n",
    "df.set_index('A', inplace=True)\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# 7. Miscellaneous\n",
    "\n",
    "# Find index of first occurrence of value\n",
    "idxmax_val = df['A'].idxmax()\n",
    "\n",
    "# Reindexing a DataFrame\n",
    "new_index = ['X', 'Y', 'Z', 'W', 'V']\n",
    "reindexed_df = df.reindex(new_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECTING COLUMNS AND ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hobbyist']\n",
    "\n",
    "df.loc[0:2, 'Hobbyist':'Employment']\n",
    "\n",
    "schema_df.loc['MgrIdiot', 'QuestionText']\n",
    "\n",
    "schema_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': ['p', 'q', 'r', 's', 't']\n",
    "})\n",
    "\n",
    "# 1. Basic Filtering\n",
    "\n",
    "# Single condition\n",
    "filtered_a = df[df['A'] > 3]\n",
    "\n",
    "# Multiple conditions using & (AND)\n",
    "filtered_and = df[(df['A'] > 2) & (df['B'] < 4)]\n",
    "\n",
    "# Multiple conditions using | (OR)\n",
    "filtered_or = df[(df['A'] > 4) | (df['B'] < 2)]\n",
    "\n",
    "# 2. Using .isin()\n",
    "\n",
    "# Filter by values in a list\n",
    "filtered_isin = df[df['C'].isin(['p', 'q'])]\n",
    "\n",
    "# 3. Using .query()\n",
    "\n",
    "# Single condition\n",
    "filtered_query = df.query(\"A > 3\")\n",
    "\n",
    "# Multiple conditions\n",
    "filtered_query_multi = df.query(\"A > 2 & B < 4\")\n",
    "\n",
    "# 4. Filtering with String Methods\n",
    "\n",
    "# Filter by string contains\n",
    "filtered_str_contains = df[df['C'].str.contains('p')]\n",
    "\n",
    "# Filter by string startswith\n",
    "filtered_str_start = df[df['C'].str.startswith('p')]\n",
    "\n",
    "# Filter by string endswith\n",
    "filtered_str_end = df[df['C'].str.endswith('t')]\n",
    "\n",
    "# 5. Using .where()\n",
    "\n",
    "# Replace values where the condition is False\n",
    "where_filtered = df['A'].where(df['A'] > 2, other=0)\n",
    "\n",
    "# 6. Filtering with `notnull()` and `isnull()`\n",
    "\n",
    "# Filter non-missing values\n",
    "filtered_notnull = df[df['A'].notnull()]\n",
    "\n",
    "# Filter missing values\n",
    "filtered_isnull = df[df['A'].isnull()]\n",
    "\n",
    "# 7. Using .duplicated()\n",
    "\n",
    "# Filter duplicate rows\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Filter unique rows (non-duplicates)\n",
    "uniques = df[~df.duplicated()]\n",
    "\n",
    "# 8. Filtering with Regex\n",
    "\n",
    "# Filter using regular expression\n",
    "regex_filtered = df[df['C'].str.match(r'p|q')]\n",
    "\n",
    "# 9. Using between()\n",
    "\n",
    "# Filter values between a range\n",
    "filtered_between = df[df['A'].between(2, 4)]\n",
    "\n",
    "# 10. Custom Functions with `.apply()`\n",
    "\n",
    "# Filter using a custom function\n",
    "def custom_filter(x):\n",
    "    return x > 2 and x < 5\n",
    "\n",
    "filtered_custom = df[df['A'].apply(custom_filter)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING AND REMOVING COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1]\n",
    "})\n",
    "\n",
    "# 1. Adding Columns\n",
    "\n",
    "# Adding a new column with scalar value\n",
    "df['C'] = 100\n",
    "\n",
    "# Adding a column as a result of an operation between other columns\n",
    "df['D'] = df['A'] + df['B']\n",
    "\n",
    "# Adding a column using `assign()`\n",
    "df = df.assign(E=df['A'] * 2)\n",
    "\n",
    "# Adding a column based on a condition\n",
    "df['F'] = df['A'].apply(lambda x: 'High' if x > 3 else 'Low')\n",
    "\n",
    "# 2. Modifying Columns\n",
    "\n",
    "# Modifying a column based on its own values\n",
    "df['A'] = df['A'] * 100\n",
    "\n",
    "# Modifying a column using another column\n",
    "df['B'] = df['B'] + df['A']\n",
    "\n",
    "# Using `applymap()` to modify all columns (e.g., multiply by 2)\n",
    "df = df.applymap(lambda x: x * 2 if isinstance(x, (int, float)) else x)\n",
    "\n",
    "# 3. Removing Columns\n",
    "\n",
    "# Removing a column using `drop()`\n",
    "df.drop('A', axis=1, inplace=True)\n",
    "\n",
    "# Removing multiple columns\n",
    "df.drop(['B', 'C'], axis=1, inplace=True)\n",
    "\n",
    "# Using `del` keyword\n",
    "del df['D']\n",
    "\n",
    "# 4. Renaming Columns\n",
    "\n",
    "# Rename one or more columns\n",
    "df.rename(columns={'E': 'E_new', 'F': 'F_new'}, inplace=True)\n",
    "\n",
    "# Setting new column names directly\n",
    "df.columns = ['G', 'H']\n",
    "\n",
    "# 5. Reordering Columns\n",
    "\n",
    "# Reordering columns in a DataFrame\n",
    "df = df[['H', 'G']]\n",
    "\n",
    "# 6. Duplicating Columns\n",
    "\n",
    "# Duplicating a column\n",
    "df['G_copy'] = df['G']\n",
    "\n",
    "# 7. Splitting Columns\n",
    "\n",
    "# Splitting a string column into multiple columns\n",
    "df['split_1'], df['split_2'] = df['H'].str.split(' ', 1).str\n",
    "\n",
    "# 8. Combining Columns\n",
    "\n",
    "# Combining multiple columns into a new column\n",
    "df['combined'] = df['G'].astype(str) + \" - \" + df['H']\n",
    "\n",
    "# 9. Changing Column Data Type\n",
    "\n",
    "# Convert a column's data type to float\n",
    "df['G'] = df['G'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HANDLING MISSING DATA NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, 5],\n",
    "    'C': ['p', np.nan, 'r', 's', 't']\n",
    "})\n",
    "\n",
    "# 1. Checking for Missing Values\n",
    "\n",
    "# Check if there are any missing values in the DataFrame\n",
    "has_missing = df.isnull().any().any()\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_per_column = df.isnull().sum()\n",
    "\n",
    "# Check for missing values in each row\n",
    "missing_per_row = df.isnull().sum(axis=1)\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# 2. Removing Missing Values\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "\n",
    "# Drop columns with any missing values\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "\n",
    "# Drop rows where all values are missing\n",
    "df_dropped_all_na = df.dropna(how='all')\n",
    "\n",
    "# Drop columns where all values are missing\n",
    "df_dropped_all_na_cols = df.dropna(axis=1, how='all')\n",
    "\n",
    "# 3. Filling Missing Values\n",
    "\n",
    "# Fill missing values with a constant (e.g., 0)\n",
    "df_filled_zero = df.fillna(0)\n",
    "\n",
    "# Fill missing values with mean of the column\n",
    "df_filled_mean = df.fillna(df.mean())\n",
    "\n",
    "# Forward fill: Use the previous value in the column to fill NaN\n",
    "df_ffill = df.ffill()\n",
    "\n",
    "# Backward fill: Use the next value in the column to fill NaN\n",
    "df_bfill = df.bfill()\n",
    "\n",
    "# Fill missing values using interpolation\n",
    "df_interpolated = df.interpolate()\n",
    "\n",
    "# Fill missing string columns with a placeholder text\n",
    "df['C'].fillna('missing', inplace=True)\n",
    "\n",
    "# 4. Replacing Values\n",
    "\n",
    "# Replace all occurrences of a specific value (e.g., -999) with NaN\n",
    "df.replace(-999, np.nan, inplace=True)\n",
    "\n",
    "# 5. Using `fillna()` with method parameter\n",
    "\n",
    "# Forward fill using a specific column as reference\n",
    "df['A'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward fill using a specific column as reference\n",
    "df['B'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# 6. Using `where` to replace values\n",
    "\n",
    "# Replace negative values with NaN\n",
    "df['A'] = df['A'].where(df['A'] >= 0, np.nan)\n",
    "\n",
    "# 7. Handling Missing Data using `groupby`\n",
    "\n",
    "# Fill NaN values in each group with the group's mean\n",
    "df['A'] = df.groupby('C')['A'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# 8. Checking if a DataFrame has no missing values\n",
    "\n",
    "# True if no missing values\n",
    "is_clean = df.notnull().all().all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUPING AND AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': ['A', 'B', 'A', 'C', 'B', 'C', 'A'],\n",
    "    'Values': [10, 20, 30, 40, 50, 60, 70],\n",
    "    'Scores': [90, 80, 70, 85, 95, 87, 78]\n",
    "})\n",
    "\n",
    "# 1. Basic Grouping\n",
    "\n",
    "# Group by a single column\n",
    "grouped = df.groupby('Category')\n",
    "\n",
    "# 2. Aggregation\n",
    "\n",
    "# Compute the mean of each group\n",
    "mean_values = grouped.mean()\n",
    "\n",
    "# Multiple aggregation functions on a single column\n",
    "multi_aggregate = grouped['Values'].agg(['mean', 'sum', 'max', 'min'])\n",
    "\n",
    "# Different aggregation functions for different columns\n",
    "custom_aggregate = grouped.agg({\n",
    "    'Values': 'mean',\n",
    "    'Scores': ['sum', 'max']\n",
    "})\n",
    "\n",
    "# 3. Iterating Over Groups\n",
    "\n",
    "for name, group in grouped:\n",
    "    print(name)\n",
    "    print(group)\n",
    "\n",
    "# 4. Selecting a Group\n",
    "\n",
    "group_a = grouped.get_group('A')\n",
    "\n",
    "# 5. Applying Multiple Aggregations\n",
    "\n",
    "# Using `agg()`\n",
    "multi_agg = grouped.agg(['mean', 'sum'])\n",
    "\n",
    "# 6. Grouping by Multiple Columns\n",
    "\n",
    "grouped_multiple = df.groupby(['Category', 'Values'])\n",
    "multi_agg_multiple = grouped_multiple.sum()\n",
    "\n",
    "# 7. Using `size()` to Get Group Sizes\n",
    "\n",
    "group_sizes = grouped.size()\n",
    "\n",
    "# 8. Using `count()` for Non-NA Counts\n",
    "\n",
    "non_na_counts = grouped.count()\n",
    "\n",
    "# 9. Grouping with Custom Functions\n",
    "\n",
    "# Group by the length of the string in the 'Category' column\n",
    "grouped_custom = df.groupby(lambda x: len(df['Category'][x]))\n",
    "custom_agg = grouped_custom.sum()\n",
    "\n",
    "# 10. `filter()` to Filter Groups\n",
    "\n",
    "# Filter groups with mean value greater than 30\n",
    "filtered_groups = grouped.filter(lambda x: x['Values'].mean() > 30)\n",
    "\n",
    "# 11. Applying Custom Functions on Groups with `apply()`\n",
    "\n",
    "def custom_function(group):\n",
    "    return group.sort_values(by='Scores', ascending=False).head(1)\n",
    "\n",
    "top_scores_per_group = grouped.apply(custom_function)\n",
    "\n",
    "# 12. Using `transform()` to Return Data with Same Index\n",
    "\n",
    "z_scores = lambda x: (x - x.mean()) / x.std()\n",
    "z_scores_by_group = grouped.transform(z_scores)\n",
    "\n",
    "# 13. Using `pivot_table()` for Grouping and Aggregation\n",
    "\n",
    "pivot = df.pivot_table(values='Scores', index='Category', aggfunc='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGING JOINING AND CONCATENATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "    'key': ['K0', 'K1', 'K2', 'K3']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "    'D': ['D0', 'D1', 'D2', 'D3'],\n",
    "    'key': ['K0', 'K1', 'K2', 'K4']\n",
    "})\n",
    "\n",
    "# 1. Merging DataFrames\n",
    "\n",
    "# Merge on a specific column (common key)\n",
    "merged = pd.merge(df1, df2, on='key')\n",
    "\n",
    "# Merge with different key names in DataFrames\n",
    "merged_diff_keys = pd.merge(df1, df2, left_on='A', right_on='C')\n",
    "\n",
    "# Merge using 'how' argument\n",
    "outer_merged = pd.merge(df1, df2, on='key', how='outer')\n",
    "inner_merged = pd.merge(df1, df2, on='key', how='inner')\n",
    "left_merged = pd.merge(df1, df2, on='key', how='left')\n",
    "right_merged = pd.merge(df1, df2, on='key', how='right')\n",
    "\n",
    "# 2. Joining DataFrames\n",
    "\n",
    "# Joining on indices\n",
    "joined = df1.join(df2, lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# Joining on keys\n",
    "key_joined = df1.join(df2.set_index('key'), on='key')\n",
    "\n",
    "# 3. Concatenating DataFrames\n",
    "\n",
    "# Concatenate DataFrames vertically (along rows)\n",
    "concatenated = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Concatenate DataFrames horizontally (along columns)\n",
    "concatenated_cols = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Concatenate with keys to create a MultiIndex\n",
    "multi_indexed = pd.concat([df1, df2], keys=['df1', 'df2'])\n",
    "\n",
    "# 4. Handling Overlapping Columns\n",
    "\n",
    "# Using 'suffixes' argument in merge\n",
    "merged_suffix = pd.merge(df1, df2, on='key', suffixes=('_left', '_right'))\n",
    "\n",
    "# 5. Merging on Multiple Keys\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "    'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "    'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "    'B': ['B0', 'B1', 'B2', 'B3']\n",
    "})\n",
    "\n",
    "df4 = pd.DataFrame({\n",
    "    'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "    'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "    'D': ['D0', 'D1', 'D2', 'D3']\n",
    "})\n",
    "\n",
    "merged_multi_keys = pd.merge(df3, df4, on=['key1', 'key2'])\n",
    "\n",
    "# 6. Using `indicator` to track source of rows\n",
    "merged_with_indicator = pd.merge(df1, df2, on='key', how='outer', indicator=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATETIME OPERATIONS / DATE TIME / DAYTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date_string': ['2023-01-01', '2023-01-02', '2023-01-03'],\n",
    "    'timestamp': ['2023-01-01 12:00:00', '2023-01-02 14:30:00', '2023-01-03 15:45:00']\n",
    "})\n",
    "\n",
    "# 1. Converting Strings to DateTime\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date_string'])\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 2. Extracting Date Components\n",
    "\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['minute'] = df['datetime'].dt.minute\n",
    "df['second'] = df['datetime'].dt.second\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# 3. DateTime Operations\n",
    "\n",
    "# Time delta (difference) between two dates\n",
    "df['time_delta'] = df['datetime'] - pd.to_datetime('2023-01-01')\n",
    "\n",
    "# Adding a time delta to a date\n",
    "df['added_days'] = df['date'] + pd.Timedelta(days=5)\n",
    "\n",
    "# 4. Filtering by Date\n",
    "\n",
    "# Filter rows by a specific date\n",
    "filtered = df[df['date'] == '2023-01-02']\n",
    "\n",
    "# Filter rows before a specific date\n",
    "before_date = df[df['date'] < '2023-01-03']\n",
    "\n",
    "# 5. Setting DateTime as Index\n",
    "\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# 6. Resampling Time Series Data\n",
    "\n",
    "# Resample to daily frequency and compute mean\n",
    "daily_resampled = df.resample('D').mean()\n",
    "\n",
    "# 7. Shifting and Lagging\n",
    "\n",
    "# Shift data forward by one period\n",
    "df['lagged'] = df['day'].shift(1)\n",
    "\n",
    "# Shift data backward by one period\n",
    "df['lead'] = df['day'].shift(-1)\n",
    "\n",
    "# 8. Rolling Windows\n",
    "\n",
    "# Calculate rolling mean over a 2-day window\n",
    "df['rolling_mean'] = df['day'].rolling(window=2).mean()\n",
    "\n",
    "# 9. Working with Time Zones\n",
    "\n",
    "# Convert naive DateTime to local time zone\n",
    "df['local_time'] = df.index.tz_localize('US/Eastern')\n",
    "\n",
    "# Convert to another time zone\n",
    "df['utc_time'] = df['local_time'].tz_convert('UTC')\n",
    "\n",
    "# 10. Periods and Period Arithmetic\n",
    "\n",
    "# Convert datetime to period (monthly frequency)\n",
    "df['month_period'] = df['date'].dt.to_period('M')\n",
    "\n",
    "# Add a month to the period\n",
    "df['next_month'] = df['month_period'] + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLYING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': ['p', 'q', 'r', 's', 't']\n",
    "})\n",
    "\n",
    "# 1. Using `.apply()` on a Series\n",
    "\n",
    "# Apply function to each element in a Series\n",
    "df['A_squared'] = df['A'].apply(lambda x: x**2)\n",
    "\n",
    "# 2. Using `.apply()` on a DataFrame\n",
    "\n",
    "# Apply function to each column (default behavior)\n",
    "column_mean = df.apply(lambda col: col.mean() if col.dtype == 'float64' else None)\n",
    "\n",
    "# Apply function to each row\n",
    "row_sum = df.apply(lambda row: row['A'] + row['B'], axis=1)\n",
    "\n",
    "# 3. Using `.applymap()` on a DataFrame\n",
    "\n",
    "# Apply function element-wise on a DataFrame\n",
    "df_string = df.applymap(str)\n",
    "\n",
    "# 4. Using `.map()` on a Series\n",
    "\n",
    "# Substitute each value in a Series\n",
    "df['C_mapped'] = df['C'].map({'p': 'apple', 'q': 'banana'})\n",
    "\n",
    "# Using a function with map\n",
    "df['A_mapped'] = df['A'].map(lambda x: x + 100)\n",
    "\n",
    "# 5. Using `.transform()` on a Grouped Object or DataFrame\n",
    "\n",
    "# Group by a column and then apply function\n",
    "grouped = df.groupby('B')\n",
    "normalized = grouped.transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Apply function directly to DataFrame columns\n",
    "df_transformed = df.transform({'A': lambda x: x**2, 'B': lambda x: x+10})\n",
    "\n",
    "# 6. Using `.agg()` for Aggregation\n",
    "\n",
    "# Apply multiple functions at once per column\n",
    "aggregated = df.agg({\n",
    "    'A': ['sum', 'mean'],\n",
    "    'B': ['min', 'max']\n",
    "})\n",
    "\n",
    "# 7. Custom Functions\n",
    "\n",
    "# Define a custom function\n",
    "def custom_function(row):\n",
    "    return row['A'] * 2 if row['B'] > 3 else row['A']\n",
    "\n",
    "df['custom_A'] = df.apply(custom_function, axis=1)\n",
    "\n",
    "# 8. Vectorized Functions\n",
    "\n",
    "# Use NumPy's vectorized functions for better performance\n",
    "import numpy as np\n",
    "df['A_log'] = np.log(df['A'])\n",
    "\n",
    "# 9. Using `.pipe()`\n",
    "\n",
    "# Define a function to be used with pipe\n",
    "def add_custom_column(data, column_name, value):\n",
    "    data[column_name] = value\n",
    "    return data\n",
    "\n",
    "# Use pipe to apply sequence of operations\n",
    "df_piped = df.pipe(add_custom_column, 'D', 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [3, 1, 2, 5, 4],\n",
    "    'B': [15, 11, 13, 12, 14],\n",
    "    'C': ['c', 'a', 'e', 'd', 'b']\n",
    "})\n",
    "\n",
    "# 1. Sorting by Index\n",
    "\n",
    "# Sort by index of DataFrame (row labels)\n",
    "df_sorted_by_index = df.sort_index()\n",
    "\n",
    "# Sort by index of DataFrame in descending order\n",
    "df_sorted_by_index_desc = df.sort_index(ascending=False)\n",
    "\n",
    "# 2. Sorting by Column Values\n",
    "\n",
    "# Sort by a single column\n",
    "df_sorted_by_A = df.sort_values(by='A')\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_sorted_by_A_C = df.sort_values(by=['A', 'C'])\n",
    "\n",
    "# Sort by a column in descending order\n",
    "df_sorted_by_A_desc = df.sort_values(by='A', ascending=False)\n",
    "\n",
    "# 3. Sorting by a Custom Function\n",
    "\n",
    "# Sort by length of string in column 'C'\n",
    "df_sorted_by_length = df.sort_values(by='C', key=lambda col: col.str.len())\n",
    "\n",
    "# 4. Sorting with NaN Values\n",
    "\n",
    "# Place NaN values at the beginning\n",
    "df_sorted_na_first = df.sort_values(by='A', na_position='first')\n",
    "\n",
    "# Place NaN values at the end (default behavior)\n",
    "df_sorted_na_last = df.sort_values(by='A', na_position='last')\n",
    "\n",
    "# 5. In-place Sorting\n",
    "\n",
    "# Sort the DataFrame in-place (modifies original DataFrame)\n",
    "df.sort_values(by='A', inplace=True)\n",
    "\n",
    "# 6. Sorting Series\n",
    "\n",
    "# Sample Series\n",
    "s = pd.Series([3, 1, 2, 5, 4], index=['c', 'a', 'e', 'd', 'b'])\n",
    "\n",
    "# Sort by index\n",
    "s_sorted_by_index = s.sort_index()\n",
    "\n",
    "# Sort by values\n",
    "s_sorted_by_values = s.sort_values()\n",
    "\n",
    "# 7. Sorting MultiIndex DataFrame\n",
    "\n",
    "# Create a MultiIndex DataFrame\n",
    "arrays = [['A', 'A', 'B', 'B'], [1, 2, 1, 2]]\n",
    "multi_index = pd.MultiIndex.from_arrays(arrays, names=('letters', 'numbers'))\n",
    "df_multi = pd.DataFrame({'data': [10, 20, 30, 40]}, index=multi_index)\n",
    "\n",
    "# Sort by specific level of MultiIndex\n",
    "df_multi_sorted = df_multi.sort_values(by=('letters', 'numbers'))\n",
    "\n",
    "# Sort by specific level using `sort_index`\n",
    "df_multi_sorted_index = df_multi.sort_index(level='numbers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISPLAY OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame for demonstration\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame(np.random.randn(10, 5), columns=list('ABCDE'))\n",
    "\n",
    "# 1. Number of Rows/Columns Displayed\n",
    "\n",
    "# Set max rows displayed in output\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Set max columns displayed in output\n",
    "pd.set_option('display.max_columns', 3)\n",
    "\n",
    "# 2. Width of Columns\n",
    "\n",
    "# Set column width to a specific number\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# 3. Floating Point Display Precision\n",
    "\n",
    "# Set decimal places for floating point numbers\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# 4. Display Large Numbers as Floats\n",
    "\n",
    "# Avoid displaying large numbers in scientific notation\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# 5. Limiting DataFrame Info Printed\n",
    "\n",
    "# Control whether the df.info() method should print the null counts\n",
    "pd.set_option('display.show_counts', True)\n",
    "\n",
    "# 6. Displaying Column Interactivity\n",
    "\n",
    "# Allow the columns to be interactive (may require Jupyter)\n",
    "pd.set_option('display.html.use_mathjax', True)\n",
    "\n",
    "# 7. Expanding DataFrame Output\n",
    "\n",
    "# Expand the output display to see more columns/rows (useful in Jupyter)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "\n",
    "# 8. Customizing Notation\n",
    "\n",
    "# Control number formatting when displaying floats\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# 9. Displaying MultiIndex Frames\n",
    "\n",
    "# Control sparseness of MultiIndex frames\n",
    "pd.set_option('display.multi_sparse', True)\n",
    "\n",
    "# 10. Displaying Large Numbers with Commas\n",
    "\n",
    "# Display large numbers with commas for thousands separators\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# 11. Resetting Options to Defaults\n",
    "\n",
    "# Reset all options to their default values\n",
    "pd.reset_option('all')\n",
    "\n",
    "# 12. Viewing Options\n",
    "\n",
    "# View the current value for an option\n",
    "max_rows = pd.get_option('display.max_rows')\n",
    "\n",
    "# 13. Setting Multiple Options\n",
    "\n",
    "# Use `context` to temporarily set multiple options within a block\n",
    "with pd.option_context('display.max_rows', 3, 'display.max_columns', 2):\n",
    "    print(df)\n",
    "\n",
    "# Note: After exiting the block, the original options are restored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISCELLANEOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': ['p', 'q', 'r', 's', 't']\n",
    "})\n",
    "\n",
    "# 1. Copying a DataFrame\n",
    "\n",
    "# Deep copy of DataFrame\n",
    "df_copy = df.copy()\n",
    "\n",
    "# 2. Dropping Duplicate Rows\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "df_no_dup_in_A = df.drop_duplicates(subset='A')\n",
    "\n",
    "# 3. DataFrame Information\n",
    "\n",
    "# Get concise summary of DataFrame\n",
    "info = df.info()\n",
    "\n",
    "# Get data types of columns\n",
    "data_types = df.dtypes\n",
    "\n",
    "# 4. Changing Data Types\n",
    "\n",
    "# Change data type of a column\n",
    "df['A'] = df['A'].astype(float)\n",
    "\n",
    "# Convert multiple columns to specific data types\n",
    "df = df.astype({'A': 'float64', 'B': 'int32'})\n",
    "\n",
    "# 5. Setting DataFrame Options\n",
    "\n",
    "# Display up to 10 rows in output\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Reset all options to defaults\n",
    "pd.reset_option('all')\n",
    "\n",
    "# 6. Handling Duplicates\n",
    "\n",
    "# Find rows with duplicate values in specific columns\n",
    "duplicates = df[df.duplicated(subset=['A', 'B'])]\n",
    "\n",
    "# 7. Sample Random Rows\n",
    "\n",
    "# Sample 3 random rows\n",
    "sampled = df.sample(n=3)\n",
    "\n",
    "# Sample 20% of rows\n",
    "fraction_sampled = df.sample(frac=0.2)\n",
    "\n",
    "# 8. Getting N Largest/Smallest Values\n",
    "\n",
    "# Get 3 largest values in column 'A'\n",
    "largest = df.nlargest(3, 'A')\n",
    "\n",
    "# Get 3 smallest values in column 'B'\n",
    "smallest = df.nsmallest(3, 'B')\n",
    "\n",
    "# 9. Memory Usage\n",
    "\n",
    "# Get memory usage of DataFrame\n",
    "memory = df.memory_usage(deep=True)\n",
    "\n",
    "# 10. Convert DataFrame to Other Formats\n",
    "\n",
    "# Convert to dictionary\n",
    "as_dict = df.to_dict()\n",
    "\n",
    "# Convert to list of records\n",
    "as_records = df.to_dict('records')\n",
    "\n",
    "# Convert to CSV string\n",
    "csv_string = df.to_csv(index=False)\n",
    "\n",
    "# 11. Convert Series to DataFrame\n",
    "\n",
    "# Convert Series to DataFrame with column name 'values'\n",
    "s_to_df = df['A'].to_frame(name='values')\n",
    "\n",
    "# 12. Check if DataFrame is Empty\n",
    "\n",
    "is_empty = df.empty\n",
    "\n",
    "# 13. Rename DataFrame Index\n",
    "\n",
    "# Rename index\n",
    "df.index.name = 'index_name'\n",
    "\n",
    "# 14. Replace Values in DataFrame\n",
    "\n",
    "# Replace specific values\n",
    "df_replaced = df.replace({'A': {1: 10, 2: 20}})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
